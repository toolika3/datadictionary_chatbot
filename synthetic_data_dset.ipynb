{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudarshan-koirala/youtube-stuffs/blob/main/langchain/synthetic_data_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "raw",
      "id": "1302a608-4b4d-46bf-bd0c-b4f13eff2e5e",
      "metadata": {
        "id": "1302a608-4b4d-46bf-bd0c-b4f13eff2e5e"
      },
      "source": [
        "# Synthetic data generation [Main LangChain Documentation](https://python.langchain.com/docs/use_cases/data_generation)\n",
        "\n",
        "## [Youtube video covering this notebook](https://youtu.be/hDTLt8UbWYg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0AtyLRnxSD2g",
      "metadata": {
        "id": "0AtyLRnxSD2g"
      },
      "source": [
        "### How Is it done ??\n",
        "### Before GenAI emerged (Well, this is used still in many companies but **GenAI is what everyone is talking about these days** )\n",
        "Some of the modelling techniques used:\n",
        "- Classic statistical methods\n",
        "- Deep Learning models (GAN , VAE behind the scene)\n",
        "- Mix of classic statistical models and Deep Learning.\n",
        "\n",
        "Once synthetic data is generated, we need to evaluate it to make sure it is OK to use in downstream tasks. There are many libraries, websites offering these kind of solutions. But here, we focus in GenAI part."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa3571cc",
      "metadata": {
        "id": "aa3571cc"
      },
      "source": [
        "## Use case\n",
        "\n",
        "Synthetic data refers to artificially generated data that imitates the characteristics of real data without containing any information from actual individuals or entities. It is typically created through mathematical models, algorithms, or other data generation techniques. Synthetic data can be used for a variety of purposes, including testing, research, and training machine learning models, while preserving privacy and security.\n",
        "\n",
        "Benefits of Synthetic Data:\n",
        "\n",
        "1. **Privacy and Security**: No real personal data at risk of breaches.\n",
        "2. **Data Augmentation**: Expands datasets for machine learning.\n",
        "3. **Flexibility**: Create specific or rare scenarios.\n",
        "4. **Cost-effective**: Often cheaper than real-world data collection.\n",
        "5. **Regulatory Compliance**: Helps navigate strict data protection laws.\n",
        "6. **Model Robustness**: Can lead to better generalizing AI models.\n",
        "7. **Rapid Prototyping**: Enables quick testing without real data.\n",
        "8. **Controlled Experimentation**: Simulate specific conditions.\n",
        "9. **Access to Data**: Alternative when real data isn't available.\n",
        "\n",
        "**Note: Despite the benefits, synthetic data should be used carefully, as it may not always capture real-world complexities.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EQ1qYxSgbDMt",
      "metadata": {
        "id": "EQ1qYxSgbDMt"
      },
      "source": [
        "## Quickstart\n",
        "\n",
        "In this notebook, we'll dive deep into generating synthetic medical billing records using the langchain library. This tool is particularly useful when you want to develop or test algorithms but don't want to use real patient data due to privacy concerns or data availability issues."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bca57012",
      "metadata": {
        "id": "bca57012"
      },
      "source": [
        "## Setup\n",
        "- First, you'll need to have the langchain library installed, along with its dependencies. Since we're using the OpenAI generator chain, we'll install that as well. Since this is an experimental lib, we'll need to include `langchain_experimental` in our installs.\n",
        "- [Pydantic](https://docs.pydantic.dev/latest/): Data validation library for Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0377478",
      "metadata": {
        "id": "a0377478"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U langchain langchain_experimental openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7bDU3j-Zb0Jf",
      "metadata": {
        "id": "7bDU3j-Zb0Jf"
      },
      "outputs": [],
      "source": [
        "# set environment variables\n",
        "# https://platform.openai.com/account/api-keys\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e3kkn3SlbzBF",
      "metadata": {
        "id": "e3kkn3SlbzBF"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.pydantic_v1 import BaseModel\n",
        "from langchain_experimental.tabular_synthetic_data.base import SyntheticDataGenerator\n",
        "from langchain_experimental.tabular_synthetic_data.openai import create_openai_data_generator, OPENAI_TEMPLATE\n",
        "from langchain_experimental.tabular_synthetic_data.prompts import SYNTHETIC_FEW_SHOT_SUFFIX, SYNTHETIC_FEW_SHOT_PREFIX"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a0917b",
      "metadata": {
        "id": "a5a0917b"
      },
      "source": [
        "## 1. Define Your Data Model\n",
        "- Every dataset has a structure or a \"schema\".\n",
        "- The MedicalBilling class below serves as our schema for the synthetic data.\n",
        "- By defining this, we're informing our synthetic data generator about the shape and nature of data we expect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "291bad6e",
      "metadata": {
        "id": "291bad6e"
      },
      "outputs": [],
      "source": [
        "class DSETData(BaseModel):\n",
        "    dataset_reg_id: str\n",
        "    dataset_name: str\n",
        "    dataset_description: str\n",
        "    dataset_type: str\n",
        "    cloud_id: str\n",
        "    data_source_system: str\n",
        "    dataset_layer: str\n",
        "    dataset_classification_type: str\n",
        "    topic: str\n",
        "    topic_category: str\n",
        "    business_owner: str\n",
        "    technical_poc: str\n",
        "    s3_buket: str\n",
        "    structured_data: str\n",
        "    cloud_database_name: str\n",
        "    table_name: str\n",
        "    file_format: str\n",
        "    refresh_type: str\n",
        "    refresh_frequency:str\n",
        "    dataset_data_retention_period: str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2059ca63",
      "metadata": {
        "id": "2059ca63"
      },
      "source": [
        "## 2. Sample Data\n",
        "To guide the synthetic data generator, it's useful to provide it with a few real-world-like examples. These examples serve as a \"seed\" - they're representative of the kind of data you want, and the generator will use them to create more data that looks similar.\n",
        "\n",
        "Here are some fictional medical billing records:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b989b792",
      "metadata": {
        "id": "b989b792"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\"example\": \"\"\"dataset_reg_id: DSET000111,\n",
        "     dataset_name: Participant Demographic Data,\n",
        "    dataset_description: This table provides comparison of demographic group prevalence in Orgs. Volunteer population to that of greater US population,\n",
        "    dataset_type: EDL,\n",
        "    cloud_id: BPO02233,\n",
        "    data_source_system: Demographics,\n",
        "    dataset_layer: S3,\n",
        "    dataset_classification_type: External,\n",
        "    topic: US Census Bureau,\n",
        "    topic_category: Demographic Analysis,\n",
        "    business_owner: toolika@gmail.com,\n",
        "    technical_poc: abc@gmail.com,\n",
        "    s3_buket: s3-us-east-1,\n",
        "    structured_data: Structured,\n",
        "    cloud_database_name: cloud_participant_demographic,\n",
        "    table_name: ameri_part_demo_grp,\n",
        "    file_format: PARQUET,\n",
        "    refresh_type: Snapshot,\n",
        "    refresh_frequency:Yearly,\n",
        "    dataset_data_retention_period: 6 years\"\"\"},\n",
        "    {\"example\": \"\"\"dataset_reg_id: DSET000112,\n",
        "     dataset_name: Loan Origination Data,\n",
        "    dataset_description: This table provides information on origination characteristics of all mortgages purchased or guaranteed by Freddie Mac with origination dates between January 1, 1999 and the Origination Cutoff Date,\n",
        "    dataset_type: EDL,\n",
        "    cloud_id: BPO02234,\n",
        "    data_source_system: Loan Origination,\n",
        "    dataset_layer: S3,\n",
        "    dataset_classification_type: External,\n",
        "    topic: Freddie Loan Population,\n",
        "    topic_category: Loan Origination Data,\n",
        "    business_owner: toolika@gmail.com,\n",
        "    technical_poc: abc@gmail.com,\n",
        "    s3_buket: s3-us-east-1,\n",
        "    structured_data: Structured,\n",
        "    cloud_database_name: freddie_origination_data,\n",
        "    table_name: fre_origination_data,\n",
        "    file_format: PARQUET,\n",
        "    refresh_type: Snapshot,\n",
        "    refresh_frequency:Yearly,\n",
        "    dataset_data_retention_period: 6 years\"\"\"},\n",
        "    {\"example\": \"\"\"dataset_reg_id: DSET000113,\n",
        "     dataset_name: Loan Performance Data,\n",
        "    dataset_description: This table provides monthly loan-level credit performance data through the Performance Cutoff Date up to property disposition is reported for all loans included in the performance files,\n",
        "    dataset_type: EDL,\n",
        "    cloud_id: BPO02235,\n",
        "    data_source_system: Loan Performance,\n",
        "    dataset_layer: S3,\n",
        "    dataset_classification_type: External,\n",
        "    topic: Freddie Loan Population,\n",
        "    topic_category: Loan Performance Data,\n",
        "    business_owner: toolika@gmail.com,\n",
        "    technical_poc: abc@gmail.com,\n",
        "    s3_buket: s3-us-east-1,\n",
        "    structured_data: Structured,\n",
        "    cloud_database_name: freddie_performance_data,\n",
        "    table_name: fre_performance_data,\n",
        "    file_format: PARQUET,\n",
        "    refresh_type: Snapshot,\n",
        "    refresh_frequency:Yearly,\n",
        "    dataset_data_retention_period: 1 years\"\"\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57e28809",
      "metadata": {
        "id": "57e28809"
      },
      "source": [
        "## 3. Craft a Prompt Template\n",
        "The generator doesn't magically know how to create our data; we need to guide it. We do this by creating a prompt template. This template helps instruct the underlying language model on how to produce synthetic data in the desired format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ea6e042e",
      "metadata": {
        "id": "ea6e042e"
      },
      "outputs": [],
      "source": [
        "OPENAI_TEMPLATE = PromptTemplate(input_variables=[\"example\"], template=\"{example}\")\n",
        "\n",
        "prompt_template = FewShotPromptTemplate(\n",
        "    prefix=SYNTHETIC_FEW_SHOT_PREFIX,\n",
        "    examples=examples,\n",
        "    suffix=SYNTHETIC_FEW_SHOT_SUFFIX,\n",
        "    input_variables=[\"subject\", \"extra\"],\n",
        "    example_prompt=OPENAI_TEMPLATE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa6da3cb",
      "metadata": {
        "id": "fa6da3cb"
      },
      "source": [
        "The `FewShotPromptTemplate` includes:\n",
        "\n",
        "- `prefix` and `suffix`: These likely contain guiding context or instructions.\n",
        "- `examples`: The sample data we defined earlier.\n",
        "- `input_variables`: These variables (\"subject\", \"extra\") are placeholders you can dynamically fill later. For instance, \"subject\" might be filled with \"medical_billing\" to guide the model further.\n",
        "- `example_prompt`: This prompt template is the format we want each example row to take in our prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pI5oGS-fchEi",
      "metadata": {
        "id": "pI5oGS-fchEi"
      },
      "source": [
        "## 4. Creating the Data Generator\n",
        "With the schema and the prompt ready, the next step is to create the data generator. This object knows how to communicate with the underlying language model to get synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1b9ba911",
      "metadata": {
        "id": "1b9ba911"
      },
      "outputs": [],
      "source": [
        "synthetic_data_generator = create_openai_data_generator(\n",
        "    output_schema=DSETData,\n",
        "    llm=ChatOpenAI(temperature=1),\n",
        "    prompt=prompt_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4198bd6",
      "metadata": {
        "id": "a4198bd6"
      },
      "source": [
        "## 5. Generate Synthetic Data\n",
        "Finally, let's get our synthetic data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a424c890",
      "metadata": {
        "id": "a424c890"
      },
      "outputs": [],
      "source": [
        "synthetic_results = synthetic_data_generator.generate(\n",
        "    subject=\"dset_data\",\n",
        "    extra=\"the name must be chosen at random. Make it something you wouldn't normally choose.\",\n",
        "    runs=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa4402e9",
      "metadata": {
        "id": "fa4402e9"
      },
      "source": [
        "This command asks the generator to produce 10 synthetic medical billing records. The results are stored in `synthetic_results`. The output will be a list of the MedicalBilling pydantic models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8lIQ7XaJeIsv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lIQ7XaJeIsv",
        "outputId": "0ea3c614-8b5b-44c0-ea93-3085b700ec60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(synthetic_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NV99vyuGc_us",
      "metadata": {
        "id": "NV99vyuGc_us"
      },
      "source": [
        "## 6. Visualize the Generated Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0b03de4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b03de4d",
        "outputId": "e65cd16a-d6ad-474b-f1ae-e58717f04891"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(synthetic_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "U4xA8dAAc-DY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4xA8dAAc-DY",
        "outputId": "1271d4b1-1eb6-476a-8377-ae1182987dd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[DSETData(dataset_reg_id='DSET000114', dataset_name='Financial Transaction Records', dataset_description='This table contains detailed records of financial transactions between various organizations and individuals, including transaction amounts, dates, and types.', dataset_type='EDL', cloud_id='BPO02236', data_source_system='Financial Transactions', dataset_layer='S3', dataset_classification_type='External', topic='Financial Transactions Analysis', topic_category='Financial Data Analysis', business_owner='anya@gmail.com', technical_poc='xyz@gmail.com', s3_buket='s3-us-east-1', structured_data='Structured', cloud_database_name='financial_transactions_db', table_name='fin_transaction_records', file_format='PARQUET', refresh_type='Incremental', refresh_frequency='Monthly', dataset_data_retention_period='5 years'),\n",
              " DSETData(dataset_reg_id='DSET000115', dataset_name='Customer Satisfaction Survey Data', dataset_description='This dataset includes responses from customer satisfaction surveys conducted quarterly to gather feedback on product and service satisfaction levels.', dataset_type='EDL', cloud_id='BPO02237', data_source_system='Customer Feedback', dataset_layer='S3', dataset_classification_type='External', topic='Customer Satisfaction Analysis', topic_category='Customer Feedback Data', business_owner='john.doe@gmail.com', technical_poc='jane.doe@gmail.com', s3_buket='s3-us-east-1', structured_data='Structured', cloud_database_name='customer_feedback_db', table_name='customer_survey_data', file_format='PARQUET', refresh_type='Incremental', refresh_frequency='Quarterly', dataset_data_retention_period='3 years'),\n",
              " DSETData(dataset_reg_id='DSET000113', dataset_name='Synthetic Marketing Data', dataset_description='This dataset contains synthetic data generated for marketing analysis and segmentation purposes.', dataset_type='EDL', cloud_id='BPO02235', data_source_system='Marketing Analytics', dataset_layer='S3', dataset_classification_type='External', topic='Marketing Analysis', topic_category='Marketing Data', business_owner='marketing.team@gmail.com', technical_poc='analytics.team@gmail.com', s3_buket='s3-us-east-1', structured_data='Structured', cloud_database_name='marketing_analytics_db', table_name='synthetic_marketing_data', file_format='PARQUET', refresh_type='Incremental', refresh_frequency='Weekly', dataset_data_retention_period='2 years'),\n",
              " DSETData(dataset_reg_id='DSET000116', dataset_name='Operational Efficiency Metrics', dataset_description='This dataset provides insights into the operational efficiency of a company, including key performance indicators and metrics related to production, logistics, and resource utilization.', dataset_type='EDL', cloud_id='BPO02238', data_source_system='Operational Efficiency Tracking', dataset_layer='S3', dataset_classification_type='External', topic='Operational Efficiency Analysis', topic_category='Business Performance Data', business_owner='operations.manager@gmail.com', technical_poc='analytics.support@gmail.com', s3_buket='s3-us-east-1', structured_data='Structured', cloud_database_name='operational_metrics_db', table_name='operational_efficiency_metrics', file_format='PARQUET', refresh_type='Incremental', refresh_frequency='Bi-weekly', dataset_data_retention_period='4 years'),\n",
              " DSETData(dataset_reg_id='DSET000118', dataset_name='Financial Fraud Detection Data', dataset_description='This dataset contains synthetic data generated for detecting financial fraud and fraudulent activities within a financial institution.', dataset_type='EDL', cloud_id='BPO02239', data_source_system='Fraud Detection System', dataset_layer='S3', dataset_classification_type='External', topic='Fraud Detection Analysis', topic_category='Financial Data', business_owner='finance.team@gmail.com', technical_poc='analytics.team@gmail.com', s3_buket='s3-us-east-1', structured_data='Structured', cloud_database_name='fraud_detection_db', table_name='financial_fraud_data', file_format='PARQUET', refresh_type='Incremental', refresh_frequency='Monthly', dataset_data_retention_period='5 years'),\n",
              " DSETData(dataset_reg_id='DSET000120', dataset_name='Enigmatic Customer Behavior Insights', dataset_description='This dataset contains synthetic data generated to analyze and understand enigmatic customer behavior patterns for strategic decision-making in marketing and sales processes.', dataset_type='EDL', cloud_id='BPO02240', data_source_system='Customer Behavior Analytics', dataset_layer='S3', dataset_classification_type='External', topic='Customer Behavior Analysis', topic_category='Marketing and Sales Data', business_owner='insights.team@gmail.com', technical_poc='analytics.support@gmail.com', s3_buket='s3-us-east-1', structured_data='Structured', cloud_database_name='customer_insights_db', table_name='customer_behavior_insights', file_format='PARQUET', refresh_type='Full', refresh_frequency='Quarterly', dataset_data_retention_period='3 years'),\n",
              " DSETData(dataset_reg_id='DSET000119', dataset_name='Innovative Supply Chain Optimization Data', dataset_description='This dataset contains synthetic data generated to optimize supply chain operations and improve efficiency in the flow of goods and services within a company.', dataset_type='EDL', cloud_id='BPO02241', data_source_system='Supply Chain Management System', dataset_layer='S3', dataset_classification_type='External', topic='Supply Chain Optimization', topic_category='Logistics Data', business_owner='supply.chain@gmail.com', technical_poc='analytics.engineer@gmail.com', s3_buket='s3-us-east-1', structured_data='Structured', cloud_database_name='supply_chain_db', table_name='supply_chain_optimization_data', file_format='PARQUET', refresh_type='Full', refresh_frequency='Bi-annual', dataset_data_retention_period='2 years'),\n",
              " DSETData(dataset_reg_id='DSET000123', dataset_name='Mystical Energy Consumption Analysis Data', dataset_description='This dataset contains synthetic data generated for analyzing mystical energy consumption patterns and trends for mystical energy sources.', dataset_type='EDL', cloud_id='BPO02242', data_source_system='Mystical Energy Analytics', dataset_layer='S3', dataset_classification_type='External', topic='Mystical Energy Consumption Analysis', topic_category='Energy Data', business_owner='energy.analytics@gmail.com', technical_poc='mystical.energy.team@gmail.com', s3_buket='s3-us-east-1', structured_data='Structured', cloud_database_name='mystical_energy_db', table_name='energy_consumption_data', file_format='PARQUET', refresh_type='Incremental', refresh_frequency='Monthly', dataset_data_retention_period='4 years'),\n",
              " DSETData(dataset_reg_id='DSET000125', dataset_name='Whimsical Textile Production Insights', dataset_description='This dataset contains synthetic data generated to analyze and understand whimsical textile production trends and insights for strategic decision-making in the fashion industry.', dataset_type='EDL', cloud_id='BPO02243', data_source_system='Textile Production Analytics', dataset_layer='S3', dataset_classification_type='External', topic='Textile Production Analysis', topic_category='Fashion Industry Data', business_owner='textile.insights@gmail.com', technical_poc='analytics.expert@gmail.com', s3_buket='s3-us-east-1', structured_data='Structured', cloud_database_name='textile_production_db', table_name='textile_production_insights', file_format='PARQUET', refresh_type='Full', refresh_frequency='Quarterly', dataset_data_retention_period='3 years'),\n",
              " DSETData(dataset_reg_id='DSET000127', dataset_name='Cryptic Market Analysis Data', dataset_description='This dataset contains synthetic data generated for analyzing cryptic market trends and patterns for mysterious financial assets and investments.', dataset_type='EDL', cloud_id='BPO02244', data_source_system='Cryptic Market Analytics', dataset_layer='S3', dataset_classification_type='External', topic='Cryptic Market Analysis', topic_category='Financial Data', business_owner='cryptic.market@gmail.com', technical_poc='data.analysis.team@gmail.com', s3_buket='s3-us-east-1', structured_data='Structured', cloud_database_name='cryptic_market_db', table_name='market_analysis_data', file_format='PARQUET', refresh_type='Incremental', refresh_frequency='Weekly', dataset_data_retention_period='5 years')]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "synthetic_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XJ3vfh9UfyLS",
      "metadata": {
        "id": "XJ3vfh9UfyLS"
      },
      "source": [
        "## 7. Converting the synthetic data into Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "LLbrrDNMeQLe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "LLbrrDNMeQLe",
        "outputId": "223f0c2b-5796-42b1-fe93-a26795ce554b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a list of dictionaries from the objects\n",
        "synthetic_data = []\n",
        "for item in synthetic_results:\n",
        "    synthetic_data.append({\n",
        "    'dataset_reg_id': item.dataset_reg_id,\n",
        "    'dataset_name': item.dataset_name,\n",
        "    'dataset_description': item.dataset_description,\n",
        "    'dataset_type': item.dataset_type,\n",
        "    'cloud_id': item.cloud_id,\n",
        "    'data_source_system': item.data_source_system,\n",
        "    'dataset_layer': item.dataset_layer,\n",
        "    'dataset_classification_type': item.dataset_classification_type,\n",
        "    'topic': item.topic,\n",
        "    'topic_category': item.topic_category,\n",
        "    'business_owner': item.business_owner,\n",
        "    'technical_poc': item.technical_poc,\n",
        "    's3_buket': item.s3_buket,\n",
        "    'structured_data': item.structured_data,\n",
        "    'cloud_database_name': item.cloud_database_name,\n",
        "    'table_name': item.table_name,\n",
        "    'file_format': item.file_format,\n",
        "    'refresh_type': item.refresh_type,\n",
        "    'refresh_frequency':item.refresh_frequency,\n",
        "    'dataset_data_retention_period': item.dataset_data_retention_period\n",
        "    })\n",
        "\n",
        "# Create a Pandas DataFrame from the list of dictionaries\n",
        "synthetic_df = pd.DataFrame(synthetic_data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(type(synthetic_df))\n",
        "synthetic_df\n",
        "synthetic_df.to_csv('DSETData_Synthetic.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3z0FY1xEe81v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z0FY1xEe81v",
        "outputId": "614c2dee-388c-4efd-b1e0-16256c2347fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 20)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "synthetic_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pYsSc2jcfr6R",
      "metadata": {
        "id": "pYsSc2jcfr6R"
      },
      "source": [
        "### Start exploring based on your usecase and use the same approach for real sensitive data. But, be careful, as the synthetic data might not capture the real-world complexities."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
